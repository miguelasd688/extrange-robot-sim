--- git status ---
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_cfg.py
	modified:   source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env.py
	modified:   source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env_cfg.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	=
	launch.sh
	self.cfg.max_base_orientation_angle
	source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_rewards.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_cfg.py b/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_cfg.py
index 94ddc35..1f24f35 100644
--- a/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_cfg.py
+++ b/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_cfg.py
@@ -27,7 +27,7 @@ EXTRANGE_ROBOT_CFG = ArticulationCfg(
         pos=(0.0, 0.0, 0.01),
         joint_pos={
             "base_to_wheel_joint": 0.0,
-            "extension_rotation_joint": 0.0,
+            "extension_rotation_joint": 1.57,
             "extension_tilt_joint": 0.0
         },
     ),
diff --git a/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env.py b/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env.py
index dc1f4fd..3480a6d 100644
--- a/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env.py
+++ b/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env.py
@@ -11,19 +11,9 @@ from isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane
 from isaaclab.utils.math import sample_uniform
 
 from .extrange_robot_env_cfg import ExtrangeRobotEnvCfg
+from .extrange_robot_rewards import CoMBalanceReward 
 
 
-def quaternion_to_euler_xyz(quat: torch.Tensor) -> torch.Tensor:
-    w, x, y, z = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]
-    sinr_cosp = 2.0 * (w * x + y * z)
-    cosr_cosp = 1.0 - 2.0 * (x * x + y * y)
-    roll = torch.atan2(sinr_cosp, cosr_cosp)
-    sinp = 2.0 * (w * y - z * x)
-    pitch = torch.asin(torch.clamp(sinp, -1.0, 1.0))
-    siny_cosp = 2.0 * (w * z + x * y)
-    cosy_cosp = 1.0 - 2.0 * (y * y + z * z)
-    yaw = torch.atan2(siny_cosp, cosy_cosp)
-    return torch.stack([roll, pitch, yaw], dim=-1)
 
 
 class ExtrangeRobotEnv(DirectRLEnv):
@@ -47,7 +37,20 @@ class ExtrangeRobotEnv(DirectRLEnv):
         #self.joint_torque = self.robot.data.joint_torque
 
         self._goal_pos = torch.tensor([1.0, 0.0, 0.0], device=self.device)
-
+        self.reward_strategy = CoMBalanceReward(self) 
+
+    def quaternion_to_euler_xyz(self, quat: torch.Tensor) -> torch.Tensor:
+        w, x, y, z = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]
+        sinr_cosp = 2.0 * (w * x + y * z)
+        cosr_cosp = 1.0 - 2.0 * (x * x + y * y)
+        roll = torch.atan2(sinr_cosp, cosr_cosp)
+        sinp = 2.0 * (w * y - z * x)
+        pitch = torch.asin(torch.clamp(sinp, -1.0, 1.0))
+        siny_cosp = 2.0 * (w * z + x * y)
+        cosy_cosp = 1.0 - 2.0 * (y * y + z * z)
+        yaw = torch.atan2(siny_cosp, cosy_cosp)
+        return torch.stack([roll, pitch, yaw], dim=-1)
+    
     def _setup_scene(self):
         self.robot = Articulation(self.cfg.robot_cfg)
         spawn_ground_plane(prim_path="/World/ground", cfg=GroundPlaneCfg())
@@ -68,16 +71,17 @@ class ExtrangeRobotEnv(DirectRLEnv):
             joint_ids=self._joint_indices
         )
 
+
     def _get_observations(self) -> dict:
         pos = torch.stack([self.joint_pos[:, i] for i in self._joint_indices], dim=-1)
         vel = torch.stack([self.joint_vel[:, i] for i in self._joint_indices], dim=-1)
 
-        #torq = torch.stack([self.joint_effort[:, i] for i in self._joint_indices], dim=-1)
-        #if self._frame_count % 60 == 0:
-        #    print(f"[DEBUG] Torques aplicados (Nm): {torq[0].cpu().numpy()}")
+#        torq = torch.stack([self.joint_effort[:, i] for i in self._joint_indices], dim=-1)
+#        if self._frame_count % 60 == 0:
+#            print(f"[DEBUG] Torques aplicados (Nm): {torq[0].cpu().numpy()}")
 
         quat = self.robot.data.root_quat_w
-        euler = quaternion_to_euler_xyz(quat)
+        euler = self.quaternion_to_euler_xyz(quat)
         ang_vel = self.robot.data.root_ang_vel_w
         lin_vel = self.robot.data.root_lin_vel_w
         dt = self.cfg.sim.dt
@@ -85,61 +89,99 @@ class ExtrangeRobotEnv(DirectRLEnv):
         self._prev_root_vel = lin_vel.clone()
         imu = torch.cat([euler, ang_vel, lin_acc], dim=-1)
 
-        #if self._frame_count % 10 == 0:
-        #    print(f"[DEBUG] Euler: {euler[0].cpu().numpy()}")
+#        if self._frame_count % 10 == 0:
+#            print(f"[DEBUG] Euler: {euler[0].cpu().numpy()}")
+#            print(f"[DEBUG] rotation actuator: {self.joint_pos[0, self._joint_indices[1]]}")
+#            print(f"[DEBUG] tilt actuator: {self.joint_pos[0, self._joint_indices[2]]}")
 
         obs = torch.cat((pos, vel, imu), dim=-1)
         return {"policy": obs}
+    
 
     def _get_rewards(self) -> torch.Tensor:
-        joint_pos = torch.stack([self.joint_pos[:, i] for i in self._joint_indices], dim=-1)
-        joint_vel = torch.stack([self.joint_vel[:, i] for i in self._joint_indices], dim=-1)
-
-        #penalize body tilt
+        return self.reward_strategy.compute()
+
+######################################################################
+#                   basic controller
+######################################################################
+#
+#        quat = self.robot.data.root_quat_w
+#        pos = self.robot.data.root_pos_w
+#        ang_vel = self.robot.data.root_ang_vel_w
+#        euler = quaternion_to_euler_xyz(quat)
+#
+#        roll = euler[:, 0]
+#        pitch = euler[:, 1]
+#        yaw = euler[:, 2]
+#
+#        # magnitud de la inclinación
+#        base_tilt = roll ** 2 + pitch ** 2
+#        joint_1_pos = self.joint_pos[:, self._joint_indices[1]]
+#        joint_2_pos = self.joint_pos[:, self._joint_indices[2]]
+#        joint_vel = torch.stack([self.joint_vel[:, i] for i in self._joint_indices], dim=-1)
+#        torque = torch.stack([self.robot.data.applied_torque[:, i] for i in self._joint_indices], dim=-1)
+#        
+#
+#        # PRIORIDAD ALTA: verticalidad
+#        upright_reward = self.cfg.rew_upright_bonus * torch.exp(-5.0 * base_tilt)
+#        base_tilt_penalty = self.cfg.rew_base_tilt_penalty * base_tilt ** 2
+#
+#        # PRIORIDAD MEDIA: corrección activa con torque
+#        torque_wheel = torque[:, 0]
+#        desired_torque = - 2 * torch.sin(pitch)
+#        torque_effect = desired_torque * torque_wheel
+#        wheel_correction_reward = self.cfg.rew_wheel_balance_correction * torque_effect
+#
+#        torque_tilt = torque[:, 2]
+#        tilt_vel = self.joint_vel[:, self._joint_indices[2]]
+#        torque_effect = torch.sign(pitch) * torque_tilt * tilt_vel
+#        tilt_correction_reward = self.cfg.rew_tilt_balance_correction * torch.tanh(roll) * torque_effect
+#
+#
+#        # PRIORIDAD BAJA: mantenerse cerca de Y = 0
+#        target_y = self.cfg.target_y_position
+#        position_penalty = self.cfg.rew_position_penalty * ((pos[:, 1] - target_y) ** 2)
+#
+#        # penalización menor por velocidad en joints 1 y 2
+#        velocity_penalty = self.cfg.rew_vel_penalty * torch.sum(joint_vel[:, 1:] ** 2, dim=-1)
+#
+#        # castigo si el joint 1 y 2 se mueve del drift
+#        target_tilt = self.cfg.target_tilt_angle
+#        target_rotation = self.cfg.target_rotation_angle
+#        tilt_target_penalty = self.cfg.rew_tilt_target_penalty * (joint_2_pos - target_tilt) ** 2
+#        rotation_target_penalty = self.cfg.rew_rotation_target_penalty * (joint_1_pos - target_rotation) ** 2
+#        
+#        alive_reward = self.cfg.rew_alive
+#
+#        total_reward = (
+#            alive_reward
+#            + upright_reward
+#            + wheel_correction_reward
+#            + tilt_correction_reward
+#            - rotation_target_penalty
+#            - tilt_target_penalty
+#            - base_tilt_penalty
+#            - position_penalty
+#            - velocity_penalty
+#        )
+    
+    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
         quat = self.robot.data.root_quat_w
-        euler = quaternion_to_euler_xyz(quat)
-        pitch = euler[:, 1]
+        euler = self.quaternion_to_euler_xyz(quat)
         roll = euler[:, 0]
-        orientation_penalty = -0.2 * (torch.abs(pitch) + torch.abs(roll))
+        pitch = euler[:, 1]
+        tilt = torch.maximum(torch.abs(roll), torch.abs(pitch))
+        out_of_bounds = tilt > self.cfg.max_base_orientation_angle
 
-        pos_penalty = -0.1 * torch.sum(joint_pos ** 2, dim=-1)
-        vel_penalty = -0.01 * torch.sum(torch.abs(joint_vel), dim=-1)
+        # detectar colisión física
+        upper_joint_z = self.robot.data.body_pos_w[:, 3, 2]
+        contact = upper_joint_z < self.cfg.min_upper_joint_height  
 
-        # Incentivar velocidad en eje X (dirección de avance)
-        lin_vel = self.robot.data.root_lin_vel_w
-        forward_reward = 1.5 * lin_vel[:, 0] 
-
-        # Stay in an objective position
-        current_pos = self.robot.data.root_pos_w
-        distance = torch.norm(current_pos - self._goal_pos, dim=-1)
-        goal_reward = -distance  # menor distancia, mayor recompensa
-
-        pos_cost = self.cfg.rew_scale_joint_pos * torch.sum(joint_pos ** 2, dim=-1)
-        vel_cost = self.cfg.rew_scale_joint_vel * torch.sum(torch.abs(joint_vel), dim=-1)
-        alive_reward = self.cfg.rew_scale_alive * (1.0 - self.reset_terminated.float())
-        death_penalty = self.cfg.rew_scale_terminated * self.reset_terminated.float()
-        reward = (
-            alive_reward + 
-            pos_cost + 
-            vel_cost + 
-            orientation_penalty + 
-            goal_reward + 
-            #forward_reward +
-            pos_penalty +
-            vel_penalty +
-            death_penalty
-        )
-        return reward 
-    
-    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
-        quat = self.robot.data.root_quat_w  # (num_envs, 4)
-        euler = quaternion_to_euler_xyz(quat)
-        pitch = euler[:, 0]  # rotación alrededor del eje X
+        time_out = self.episode_length_buf >= self.max_episode_length - 1
 
+        done = out_of_bounds | contact 
 
-        out_of_bounds = torch.abs(pitch) > self.cfg.max_base_orientation_angle
-        time_out = self.episode_length_buf >= self.max_episode_length - 1
-        return out_of_bounds, time_out
+        return done, time_out
 
     def _reset_idx(self, env_ids: Sequence[int] | None):
         if env_ids is None:
diff --git a/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env_cfg.py b/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env_cfg.py
index ff9b2ff..5e6ce81 100644
--- a/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env_cfg.py
+++ b/source/extrange_robot/extrange_robot/tasks/direct/extrange_robot/extrange_robot_env_cfg.py
@@ -1,3 +1,4 @@
+import numpy as np
 from isaaclab.envs import DirectRLEnvCfg
 from isaaclab.scene import InteractiveSceneCfg
 from isaaclab.sim import SimulationCfg
@@ -45,11 +46,43 @@ class ExtrangeRobotEnvCfg(DirectRLEnvCfg):
     action_scale = 1.0
 
     # reward parameters
-    rew_scale_alive = 1.0
-    rew_scale_terminated = -5.0
-    rew_scale_joint_pos = -0.1
-    rew_scale_joint_vel = -0.01
+    rew_upright_bonus = 0.2
+    rew_yaw_balance_correction = 0.5
+    rew_wheel_balance_correction = 0.8
+    rew_tilt_balance_correction = 0.02
+
+    rew_com_projection_penalty = 1.0
+    rew_base_tilt_penalty = 2.0
+    rew_vel_penalty = 0.05
+    rew_rotation_target_penalty = 0.5
+    rew_tilt_target_penalty = 0.5
+
+    rew_alive = 0.02
+
+    target_rotation_angle = np.pi / 2
+    target_tilt_angle = 0.0
+#    # --- rewards ---
+#    rew_upright_bonus = 1.0                # ↑ incentivo fuerte por mantener verticalidad
+#    rew_wheel_balance_correction = 8.0          # recompensa para mantener los actuadores en la posición target
+#    rew_tilt_balance_correction = 8.0
+#        
+#    rew_alive = 0.050                         # ← recompensa base por estar en pie
+#    
+#    # --- Penalizaciones  ---
+#    rew_tilt_target_penalty = 1.0
+#    rew_rotation_target_penalty = 1
+#    
+#    rew_base_tilt_penalty = 15.0                 # castigo por inclinación (ya lo premias con upright)
+#    rew_vel_penalty = 0.1                # valor bajo permite moverse sin miedo
+#    rew_position_penalty = 0.0             # ← no bloquees la exploración espacial
+#    
+#    # target positions
+#    target_y_position = 0.0                
+#    target_tilt_angle = 0.0
+#    target_rotation_angle = 1.57
+
 
     # reset conditions
-    initial_joint_pos_range = [-0.5, 0.5]
-    max_base_orientation_angle = 1.40
+    initial_joint_pos_range = [-0.1, 0.1]
+    max_base_orientation_angle = 1.20
+    min_upper_joint_height = 0.18
\ No newline at end of file